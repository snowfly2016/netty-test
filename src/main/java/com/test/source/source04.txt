Netty如何实现内存管理？
  Netty 内存管理机制，基于 Jemalloc 算法。
  http://www.cnhalo.net/2016/06/13/memory-optimize/
1.首先会预申请一大块内存 Arena ，Arena 由许多 Chunk 组成，而每个 Chunk 默认由2048个page组成。
2.Chunk 通过 AVL 树的形式组织 Page ，每个叶子节点表示一个 Page ，而中间节点表示内存区域，节点自己记录它在整个 Arena 中的偏移地址。当区域被分配出后
，中间节点上的标记位会被标记，这样就表示这个中间节点以下的所有节点都已被分配了。大于 8k 的内存分配在 PoolChunkList 中，而 PoolSubpage 用于分配小于
8k 的内存，它会把一个 page 分割成多段，进行内存分配。

********************************************************************************************************************************
Netty为什么要实现内存管理？

在 Netty 中，IO 读写必定是非常频繁的操作，而考虑到更高效的网络传输性能，Direct ByteBuffer 必然是最合适的选择。但是 Direct ByteBuffer的申请和释放
是高成本的操作，那么进行池化管理，多次重用是比较有效的方式。但是，不同于一般于我们常见的对象池、连接池等池化的案例，ByteBuffer 是有大小一说。又但是，申
请多大的 Direct ByteBuffer 进行池化又会是一个大问题，太大会浪费内存，太小又会出现频繁的扩容和内存复制！！！所以呢，就需要有一个合适的内存管理算法，
解决高效分配内存的同时又解决内存碎片化的问题。

官方解释：
Netty 4.x 增加了 Pooled Buffer，实现了高性能的 buffer 池，分配策略则是结合了 buddy allocation 和 slab allocation 的 jemalloc变种，代码在
io.netty.buffer.PoolArena 中。
官方说提供了以下优势：
频繁分配、释放 buffer 时减少了 GC 压力。
在初始化新 buffer 时减少内存带宽消耗( 初始化时不可避免的要给buffer数组赋初始值 )。
及时的释放 direct buffer 。

C/C++ 和 java 中有个围城，城里的想出来，城外的想进去！** 这个围城就是自动内存管理！
Netty 4 buffer 介绍
Netty4 带来一个与众不同的特点是其 ByteBuf 的实现，相比之下，通过维护两个独立的读写指针， 要比 io.netty.buffer.ByteBuf 简单不少，也会更高效一些。不
过，Netty 的 ByteBuf 带给我们的最大不同，就是他不再基于传统 JVM 的 GC 模式，相反，它采用了类似于 C++ 中的 malloc/free 的机制，需要开发人员来手动管
理回收与释放。从手动内存管理上升到GC，是一个历史的巨大进步， 不过，在20年后，居然有曲线的回归到了手动内存管理模式，正印证了马克思哲学观：社会总是在螺旋
式前进的，没有永远的最好。
① GC 内存管理分析
的确，就内存管理而言，GC带给我们的价值是不言而喻的，不仅大大的降低了程序员的心智包袱， 而且，也极大的减少了内存管理带来的 Crash 困扰，为函数式编程（大
量的临时对象）、脚本语言编程带来了春天。 并且，高效的GC算法也让大部分情况下程序可以有更高的执行效率。 不过，也有很多的情况，可能是手工内存管理更为合适的
。譬如：对于类似于业务逻辑相对简单，譬如网络路由转发型应用（很多erlang应用其实是这种类型）， 但是 QPS 非常高，比如1M级，在这种情况下，在每次处理中即便
产生1K的垃圾，都会导致频繁的GC产生。 在这种模式下，erlang 的按进程回收模式，或者是 C/C++ 的手工回收机制，效率更高。
Cache 型应用，由于对象的存在周期太长，GC 基本上就变得没有价值。所以，理论上，尴尬的GC实际上比较适合于处理介于这 2 者之间的情况： 对象分配的频繁程度相
比数据处理的时间要少得多的，但又是相对短暂的， 典型的，对于OLTP型的服务，处理能力在 1K QPS 量级，每个请求的对象分配在 10K-50K 量级， 能够在 5-10s
的时间内进行一 次younger GC ，每次GC的时间可以控制在 10ms 水平上， 这类的应用，实在是太适合 GC 行的模式了，而且结合 Java 高效的分代GC，简直就是一个
理想搭配。
② 影响
Netty 4 引入了手工内存的模式，我觉得这是一大创新，这种模式甚至于会延展， 应用到 Cache 应用中。实际上，结合 JVM 的诸多优秀特性，如果用 Java来实现一个
Redis 型 Cache、 或者 In-memory SQL Engine，或者是一个 Mongo DB，我觉得相比 C/C++ 而言，都要更简单很多。 实际上，JVM 也已经提供了打通这种技术的
机制，就是 Direct Memory 和 Unsafe 对象。 基于这个基础，我们可以像 C 语言一样直接操作内存。实际上，Netty4 的 ByteBuf 也是基于这个基础的。

********************************************************************************************************************************
Netty自己实现ByteBuf有什么优点呢？
A01. 它可以被用户自定义的缓冲区类型扩展
A02. 通过内置的符合缓冲区类型实现了透明的零拷贝
A03. 容量可以按需增长
A04. 在读和写这两种模式之间切换不需要调用 #flip() 方法
A05. 读和写使用了不同的索引
A06. 支持方法的链式调用
A07. 支持引用计数
A08. 支持池化

********************************************************************************************************************************
Netty如何实现重连接？
客户端，通过 IdleStateHandler 实现定时检测是否空闲，例如说 15 秒。如果空闲，则向服务端发起心跳。如果多次心跳失败，则关闭和服务端的连接，然后重新发起
连接。
服务端，通过 IdleStateHandler 实现定时检测客户端是否空闲，例如说 90 秒。如果检测到空闲，则关闭客户端。注意，如果接收到客户端的心跳请求，要反馈一个心
跳响应给客户端。通过这样的方式，使客户端知道自己心跳成功。

********************************************************************************************************************************
原生的 NIO 存在 Epoll Bug 是什么？Netty 是怎么解决的？

Java NIO Epoll BUG
Java NIO Epoll 会导致 Selector 空轮询，最终导致 CPU 100% 。官方声称在 JDK 1.6 版本的 update18 修复了该问题，但是直到 JDK 1.7版本该问题仍旧存在
，只不过该 BUG 发生概率降低了一些而已，它并没有得到根本性解决。

Netty 解决方案
对 Selector 的 select 操作周期进行统计，每完成一次空的 select 操作进行一次计数，若在某个周期内连续发生 N 次空轮询，则判断触发了 Epoll 死循环 Bug 。
（此处空的 select 操作的定义是，select 操作执行了 0 毫秒。）此时，Netty 重建 Selector 来解决。判断是否是其他线程发起的重建请求，若不是则将原
SocketChannel 从旧的 Selector 上取消注册，然后重新注册到新的 Selector 上，最后将原来的 Selector 关闭。

********************************************************************************************************************************
Netty零拷贝的实现？
Netty 的零拷贝实现，是体现在多方面的，主要如下：

1.【重点】Netty 的接收和发送 ByteBuffer 采用堆外直接内存 Direct Buffer 。使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝；使用堆
内内存会多了一次内存拷贝，JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中。
2.Netty 创建的 ByteBuffer 类型，由 ChannelConfig 配置。而 ChannelConfig 配置的 ByteBufAllocator 默认创建 Direct Buffer 类型。
3.CompositeByteBuf 类，可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf ，避免了传统通过内存拷贝的方式将几个小 Buffer 合并成一个大的 Buffer 。
#addComponents(...) 方法，可将 header 与 body 合并为一个逻辑上的 ByteBuf 。这两个 ByteBuf 在CompositeByteBuf 内部都是单独存在的，即
CompositeByteBuf 只是逻辑上是一个整体。
4.通过 FileRegion 包装的 FileChannel 。#tranferTo(...) 方法，实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel ，避免了传统通过循环
write 方式，导致的内存拷贝问题。
5.通过 wrap 方法, 我们可以将 byte[] 数组、ByteBuf、ByteBuffer 等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作。

********************************************************************************************************************************
了解哪几种序列化协议

概念：
序列化（编码），是将对象序列化为二进制形式（字节数组），主要用于网络传输、数据持久化等。
反序列化（解码），则是将从网络、磁盘等读取的字节数组还原成原始对象，主要用于网络传输对象的解码，以便完成远程调用。

选型：
在选择序列化协议的选择，主要考虑以下三个因素：
1.序列化后的字节大小。更少的字节数，可以减少网络带宽、磁盘的占用。
2.序列化的性能。对 CPU、内存资源占用情况。
3.是否支持跨语言。例如，异构系统的对接和开发语言切换。

方案：
1.Java 默认提供的序列化
无法跨语言；序列化后的字节大小太大；序列化的性能差。
2.XML
优点：人机可读性好，可指定元素或特性的名称。
缺点：序列化数据只包含数据本身以及类的结构，不包括类型标识和程序集信息；只能序列化公共属性和字段；不能序列化方法；文件庞大，文件格式复杂，传输占带宽。
适用场景：当做配置文件存储数据，实时数据转换。
3.JSON ，是一种轻量级的数据交换格式。
优点：兼容性高、数据格式比较简单，易于读写、序列化后数据较小，可扩展性好，兼容性好。与 XML 相比，其协议比较简单，解析速度比较快。
缺点：数据的描述性比 XML 差、不适合性能要求为 ms 级别的情况、额外空间开销比较大。
适用场景（可替代 XML ）：跨防火墙访问、可调式性要求高、基于Restful API 请求、传输数据量相对小，实时性要求相对低（例如秒级别）的服务。
4.Thrift ，不仅是序列化协议，还是一个 RPC 框架
优点：序列化后的体积小, 速度快、支持多种语言和丰富的数据类型、对于数据字段的增删具有较强的兼容性、支持二进制压缩编码。
缺点：使用者较少、跨防火墙访问时，不安全、不具有可读性，调试代码时相对困难、不能与其他传输层协议共同使用（例如 HTTP）、无法支持向持久层直接读写数据，即
不适合做数据持久化序列化协议。
适用场景：分布式系统的 RPC 解决方案。
5.Avro ，Hadoop 的一个子项目，解决了JSON的冗长和没有IDL的问
优点：支持丰富的数据类型、简单的动态语言结合功能、具有自我描述属性、提高了数据解析速度、快速可压缩的二进制数据形式、可以实现远程过程调用RPC、支持跨编程
语言实现。
缺点：对于习惯于静态类型语言的用户不直观。
适用场景：在 Hadoop 中做 Hive、Pig 和 MapReduce 的持久化数据格式。
6.Protobuf ，将数据结构以 .proto 文件进行描述，通过代码生成工具可以生成对应数据结构的 POJO 对象和 Protobuf 相关的方法和属性。
优点：序列化后码流小，性能高、结构化数据存储格式（XML JSON等）、通过标识字段的顺序，可以实现协议的前向兼容、结构化的文档更容易管理和维护。
缺点：需要依赖于工具生成代码、支持的语言相对较少，官方只支持Java 、C++、python。
适用场景：对性能要求高的 RPC 调用、具有良好的跨防火墙的访问属性、适合应用层对象的持久化。

【重点】Protostuff ，基于 Protobuf 协议，但不需要配置proto 文件，直接导包即可。目前，微博 RPC 框架 Motan 在使用它。
【了解】Jboss Marshaling ，可以直接序列化 Java 类，无须实 java.io.Serializable 接口。
【了解】Message Pack ，一个高效的二进制序列化格式。
【重点】Hessian ，采用二进制协议的轻量级 remoting on http 服务。目前，阿里 RPC 框架 Dubbo 的默认序列化协议。
【重要】kryo ，是一个快速高效的Java对象图形序列化框架，主要特点是性能、高效和易用。该项目用来序列化对象到文件、数据库或者网络。目前，阿里 RPC 框架
Dubbo 的可选序列化协议。
【重要】FST ，fast-serialization 是重新实现的 Java 快速对象序列化的开发包。序列化速度更快（2-10倍）、体积更小，而且兼容 JDK 原生的序列化。要求
JDK 1.7 支持。目前，阿里 RPC 框架 Dubbo 的可选序列化协议。

********************************************************************************************************************************
TCP 粘包 / 拆包的原因？应该这么解决？

概念：
TCP 是以流的方式来处理数据，所以会导致粘包 / 拆包。
拆包：一个完整的包可能会被 TCP 拆分成多个包进行发送。
粘包：也可能把小的封装成一个大的数据包发送。

原因：
应用程序写入的字节大小大于套接字发送缓冲区的大小，会发生拆包现象。而应用程序写入数据小于套接字缓冲区大小，网卡将应用多次写入的数据发送到网络上，这将会发
生粘包现象。
待发送数据大于 MSS（最大报文长度），TCP 在传输前将进行拆包。
以太网帧的 payload（净荷）大于 MTU（默认为 1500 字节）进行 IP 分片拆包。
接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包。

解决方案：
在 Netty 中，提供了多个 Decoder 解析类，如下：
① FixedLengthFrameDecoder ，基于固定长度消息进行粘包拆包处理的。
② LengthFieldBasedFrameDecoder ，基于消息头指定消息长度进行粘包拆包处理的。
③ LineBasedFrameDecoder ，基于换行来进行消息粘包拆包处理的。
④ DelimiterBasedFrameDecoder ，基于指定消息边界方式进行粘包拆包处理的。
实际上，上述四个 FrameDecoder 实现可以进行规整：
① 是 ② 的特例，固定长度是消息头指定消息长度的一种形式。
③ 是 ④ 的特例，换行是于指定消息边界方式的一种形式。